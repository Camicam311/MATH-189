---
title: "Math 189/289 Lab 4"
author: "Jiaqi Guo and Alex L"
output:
  html_notebook: default
  pdf_document: default
---

# Review of bootstrap
# Create Data
Let's say today we are going to work with some simulated data. We generate $N$ numbers from bernoulli distribution with success rate 0.3, and take them as our population.
```{r}
N <- 1000
data.population <- rbinom(n=N, size=1, prob=0.3)
```

One can give a story line to this data. If put in the same context as the video games dataset that we have for homework, for example, this could be the response of the students in the whole school whether they played games in the past week or not. Thus, the school has 1000 students in total. 1 indicates the student played, and 0 indicates did not play.

Then in that sense, we might not have observed every one of these responses as in for the survey data in the video games dataset. Thus, we sample, say $n$, observations from the population, and take them as our sample units.
```{r}
n <- 300
data.sample <- sample(data.population, size = n, replace = FALSE)
```

# Sample Statistics
Now we will stick with the story line. Once we have the sample data, we can compute a point estimate, as well as an interval estimate for the fraction of the students who played games in the past week or not.

A point estimate in this case is just the mean of the sample,
```{r}
mean.sample <- mean(data.sample)
mean.sample
```

To get an interval estimate for the fraction, we follow the derivation in the lecture slides, the interval estimate is then given by,
$$ \left(\bar x - 1.96 \sqrt{\frac{\bar x (1 - \bar x)}{n-1} \frac{N - n}{N}}, \bar x + 1.96 \sqrt{\frac{\bar x (1 - \bar x)}{n-1} \frac{N - n}{N}} \right), $$
where $\bar x$ indicates the sample mean, $N$ indicates the population size, and $n$ indicates the sample size. Thus, an interval estimate is then given as the following.
```{r}
width <- 1.96 * sqrt(mean.sample*(1-mean.sample)*(N-n)/((n-1)*N))
int.sample <- c(mean.sample - width, mean.sample + width)
int.sample
```

# Bootstrap Estimate
Another popular method is using the idea of bootstrap. We need a bootstrap population to start with. With the population size of $N=1000$, and sample size of $n=300$, approximately each sample occurs about 3 times in the bootstrap population. One can simply duplicate each observation in the sample 3 times, and treat the resulting sample as the bootstrap population.
```{r}
boot.population <- rep(data.sample, length.out = N)
data.boot <- sample(boot.population, size = n, replace = FALSE)
```

With the bootstrap population, we are now ready to generate bootstrap sample means. We will take, say 2000, random samples of size $n$.
```{r}
B <- 2000
boot.sample.mean <- rep(NA, B)
for(i in 1:B){
  boot <- sample(boot.population, size = n, replace = FALSE)
  boot.sample.mean[i] <- mean(boot)
}
```

Let's take a look at the distribution of bootstrap sample means. 
```{r}
hist(boot.sample.mean, breaks = 20)
```

The point estimate is the mean of bootstrap sample means.
```{r}
mean.boot <- mean(boot.sample.mean)
mean.boot
```

To derive an interval estimate using the bootstrap sample means, one can simply extract the 0.025-quantile and 0.975 quantile of the bootstrap sample means and arrive at an interval estimate.
```{r}
int.boot <- c(quantile(boot.sample.mean, 0.025), quantile(boot.sample.mean, 0.975))
int.boot
```

# Comparing Two Distributions
To compare two distributions, the Kolmogorov-Smirnov (KS) statistics is a helpful measure. For example, say if we take another sample from the population data earlier, and compare it with the earlier sample we took.
```{r}
data.sample2 <- sample(data.population, size = n, replace = FALSE)
ks.test(data.sample, data.sample2)
```

Now let's take another sample that is completely different.
```{r}
data.sample2 <- rbinom(n=n, size=1, prob=0.7)
ks.test(data.sample, data.sample2)
```

# Important Features
Often we woule like to pick out important reasons resulting an outcome. For example, we would like to figure out what are the import reasons why students like/dislike video games.

We first load the data.
```{r}
data <- read.table("videodata.txt", header=TRUE)
```

The package we are going to introduce today is named 'tree'.
```{r}
install.packages("tree")
library(tree)
```

Since our task only ask for like or dislike, we need to congregate the column 'like' into 2 categories from 5, as a new column named 'dis_like'. We will denote 1=never played, 4=not really, and 5=not at all of the column 'like' as 0=dislike in the new column 'dis/like', and the rest as 1=like.
```{r}
data['dis_like'] <- rep(NA, dim(data)[1])
for(i in 1:dim(data)[1]){
  like <- data[i, 'like']
  if(like==0 || like==4 || like==5){
    data[i, 'dis_like'] = 0
  }else{
    data[i, 'dis_like'] = 1
  }
}
```

We then fit a tree on the data frame with appropriate columns.
```{r}
data.tree <- tree(dis_like~educ+sex+age+home+math+work+own+cdrom+grade, data=data)
plot(data.tree, type="uniform")
text(data.tree)
```

# BART (Bayesian Additive Regression Trees)
## Model:
$$Y_i = f(X_i) + \epsilon_i, \epsilon_i \sim \mathcal{N}(0, \sigma^2)$$
simulation training data
```{r}
sigma = 0.1
f = function(x) {x^3}
set.seed(189)
n = 200
x = sort(2*runif(n)-1) # x in [-1,1]
y = f(x) + sigma*rnorm(n)
# xtest: values we want to estimate f(x) at
xtest = seq(-1,1,by=.2)
```

plot the data
```{r}
plot(x,y,cex=.5)
points(xtest,rep(0,length(xtest)),col="red",pch=16,cex=.8)
```

Now we run BART on the simulated data using the function BART::wbart.
```{r}
# install.packages('BART')
library(BART)
set.seed(189) #it is MCMC, set the seed!!
# Convert x and xtest to data matrix, i.e. with (as usual) rows corresponding to observations and columns to variables.
x <- as.matrix(x, nrow = length(x))
xtest <- as.matrix(xtest, nrow = length(xtest))
rb = wbart(x.train = x, y.train = y, x.test = xtest, nskip=500, ndpost=2000)
```
nskip: the number of draws (MCMC iterations) that will be treated as burn-in, default is 500.
ndpost: results will be kept after burn-in. Default is 1,000.

Let's see what is in rb: the list containing the results of the call to wbart.
```{r}
names(rb)
```

```{r}
dim(rb$yhat.test)
```
The (d, j) element of yhat.test is the d-th draw of f evaluated at the j-th value of xtest.
2,000 draws of f, each of which is evaluated at 11 xtest values.

Let’s have a look at the fit and uncertainty
```{r}
plot(x,y,cex=.3,cex.axis=.8,cex.lab=.7, mgp=c(1.3,.3,0),tcl=-.2,pch=".") # data
lines(xtest,f(xtest),col="blue",lty=1) # truth
lines(xtest,apply(rb$yhat.test,2,mean),col="red",lwd=1.5,lty=2) # post mean of $f(x_j)$
qm = apply(rb$yhat.test,2,quantile,probs=c(.025,.975)) # post quantiles
lines(xtest,qm[1,],col="grey",lty=1,lwd=1.0)
lines(xtest,qm[2,],col="grey",lty=1,lwd=1.0)
legend("topleft",legend=c("true f","post mean of f","95% intervals"),
col=c("blue","red","grey"), lwd=c(2,2,2), lty=c(1,2,1),bty="n",cex=.5,seg.len=3)
```

```{r}
dim(rb$yhat.train)
```
The (d, j) element of yhat.train is the d-th draw of f evaluated at the j-th value of training data x.

wbart has the following BART prior related parameters:
T: power, base.
M: k, sigmaf.
σ: sigest, sigdf, sigquant, lambda

In addition, you must choose ntree and numcut.
numcut is the number of cutpoints c used for the decision rules xi < c.

Let’s just look at a few examples to get some intuition for the effect of
these prior choices on the BART inference.

```{r}
bfb1 = wbart(x,y,sigest=.01,sigdf=500) #sigma small => overfit
bfb2 = wbart(x,y,sigest=3,sigdf=500) #sigma big => underfit
bfg = wbart(x,y,sigest=sigma,sigdf=5000) #got sigma right!
```

```{r}
plot(x,y)
lines(x,bfb1$yhat.train.mean,col="red")
lines(x,bfb2$yhat.train.mean,col="green")
lines(x,bfg$yhat.train.mean,col="blue")
```

# real data example
```{r}
db = read.csv("http://www.rob-mcculloch.org/data/diabetes.csv")
#first col is y, 2-11 are x, 12-65 are xi^2, x_i * x_j (except sex dummy)
xB = as.matrix(db[,2:11]) #x for BART
yB = db$y #y for BART
```
Note that all the predictors have been standardized. But this does not matter for BART actually (we are using trees!).

```{r}
rmsef = function(y,yhat) {return(sqrt(mean((y-yhat)^2)))}
nd=30 #number of train/test splits
n = length(yB)
ntrain=floor(.75*n) #75% in train each time
rmseB = rep(0,nd) #BART rmse
fmatB=matrix(0.0,n-ntrain,nd) #out-of-sample BART predictions
```

```{r}
for(i in 1:nd) {
  set.seed(i)
  # train/test split
  ii=sample(1:n,ntrain)
  #y
  yTrain = yB[ii]; yTest = yB[-ii]
  #x for BART
  xBTrain = xB[ii,]; xBTest = xB[-ii,]
  #BART
  bfTrain = mc.wbart(xBTrain,yTrain,xBTest,mc.cores=8,keeptrainfits=FALSE)
  #get predictions on test
  yBhat = bfTrain$yhat.test.mean
  #store results
  rmseB[i] = rmsef(yTest,yBhat)
  fmatB[,i]=yBhat
}
```

